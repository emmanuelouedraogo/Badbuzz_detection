# Bad Buzz Detection üß†‚ú®

Ce projet est une application web d'analyse de sentiments qui utilise un mod√®le de deep learning (Bidirectional GRU) pour classifier un texte en **Positif** ou **N√©gatif**.

L'application est compos√©e de deux parties :
1.  Une **API backend (Flask)** qui sert le mod√®le de machine learning.
2.  Une **interface utilisateur frontend (Streamlit)** qui permet d'interagir avec l'API.



## üèõÔ∏è Architecture

-   **Frontend** : Streamlit - Fournit une interface web interactive et moderne.
-   **Backend** : Flask - Une micro-framework web pour exposer le mod√®le via une API REST.
-   **Mod√®le ML** : Un mod√®le Keras (`.keras`) entra√Æn√© pour l'analyse de sentiments.
-   **Serveur de Production** : Gunicorn - Un serveur WSGI pour ex√©cuter l'application Flask en production.
-   **H√©bergement** : Azure App Service - Pour le d√©ploiement cloud de l'API.

## üöÄ Installation Locale

Suivez ces √©tapes pour lancer le projet sur votre machine locale.

### Pr√©requis

-   Python 3.8+
-   Git

### 1. Cloner le d√©p√¥t

```bash
git clone <url-du-depot>
cd badbuzz_detection
```

### 2. Cr√©er un environnement virtuel

Il est fortement recommand√© d'utiliser un environnement virtuel.

```bash
# Windows
python -m venv badbuzzenv
badbuzzenv\Scripts\activate

# macOS / Linux
python3 -m venv badbuzzenv
source badbuzzenv/bin/activate
```

### 3. Installer les d√©pendances

Installez toutes les biblioth√®ques n√©cessaires √† partir du fichier `requirements.txt`.

```bash
pip install -r requirements.txt
```

### 4. Placer les fichiers du mod√®le

Assurez-vous que votre mod√®le et votre tokenizer sont plac√©s correctement dans le projet :

```
badbuzz_detection/
‚îú‚îÄ‚îÄ saved_model/
‚îÇ   ‚îî‚îÄ‚îÄ best_gensim_bidirectional_gru_en_model.keras  <-- VOTRE MOD√àLE ICI
‚îú‚îÄ‚îÄ tokenizer.pickle                                  <-- VOTRE TOKENIZER ICI
‚îú‚îÄ‚îÄ app.py
‚îî‚îÄ‚îÄ streamlit_app.py
```

## ‚ñ∂Ô∏è Lancement de l'application

Vous devez lancer deux processus dans deux terminaux distincts.

### 1. Lancer l'API Flask

Dans le premier terminal (avec l'environnement virtuel activ√©) :

```bash
python app.py
```

Le serveur d√©marrera sur `http://127.0.0.1:5000`. Vous devriez voir les logs indiquant que le mod√®le et le tokenizer ont √©t√© charg√©s.

### 2. Lancer l'interface Streamlit

Dans un second terminal (avec l'environnement virtuel activ√©) :

```bash
streamlit run streamlit_app.py
```

Votre navigateur devrait s'ouvrir automatiquement sur l'interface de l'application.

## ‚òÅÔ∏è D√©ploiement sur Azure App Service

L'API Flask est con√ßue pour √™tre d√©ploy√©e sur des services cloud comme Azure App Service.

1.  **Pr√©paration** : Assurez-vous que `gunicorn` est dans `requirements.txt`.
2.  **Cr√©ation des ressources** : Utilisez Azure CLI pour cr√©er un Groupe de Ressources, un Plan App Service et une Web App.
3.  **Configuration** : Configurez la commande de d√©marrage de l'application sur Azure :
    ```bash
    az webapp config set --resource-group <rg-name> --name <app-name> --startup-file "gunicorn --bind=0.0.0.0 --timeout 600 app:app"
    ```
4.  **D√©ploiement** : D√©ployez le code via Git local en "pushant" sur le remote Azure.
    ```bash
    git push azure main
    ```

## üìù Documentation de l'API

### Endpoint de pr√©diction

-   **URL** : `/predict`
-   **M√©thode** : `POST`
-   **Description** : Analyse le sentiment du texte fourni.

#### Requ√™te
-   **Headers** : `Content-Type: application/json`
-   **Body** (raw JSON) :
  ```json
  {
    "text": "This was a fantastic experience!"
  }
  ```

#### R√©ponse (Succ√®s)
-   **Code** : `200 OK`
-   **Body** :
  ```json
  {
    "prediction": "Positive",
    "confidence_score": 0.0123
  }
  ```
  *Note : Le `confidence_score` est le score brut du mod√®le. Un score proche de 0 est "Positif", un score proche de 1 est "N√©gatif".*
